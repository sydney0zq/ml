###Cross Entropy Cost

<http://neuralnetworksanddeeplearning.com/chap3.html>

理想情况是让神经网络学习更快。

假设简单模型：只有一个输入，一个神经元，一个输出。

当我们初始值设置的和真实的值差距比较大的时候，就可以发现需要的 Epochs 更多，这也就说明了选择初始权重的重要性。

**神经网络的学习行为和人类差很多，开始学习很慢，后来逐渐增快。**

学习慢的原因就是偏导数（对于 w 和 b 的偏导）的值比较小。PDF 中的例子说的很清楚，不理解去查阅。


<hr>

cross-entropy 函数可以加快这种收敛的速度。

为什么可以用来做cost函数?

- 其值总是大于0的。
- 在a = y的时候, cost = 0。

求偏导之后得到的结果和一系列公式查阅PDF。
[NN-CH3.pdf](../engine/nn_ch3.pdf)

好处是错误大时, 更新多, 学得快; 错误小的时候, 学得慢。这样就不会有开始学习很慢的情况, 再次演示就发现速度很快。在一开始错误大的时候学习速度非常快!

学习率不是重点, 主要是速度的变化率, 也就是曲线的形状不同。


总结:

cross-entropy cost几乎总是比二次cost函数要好。

如果神经元的方程是线性的, 那么用二次cost函数不会有学习慢的问题。



