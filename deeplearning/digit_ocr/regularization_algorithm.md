##Regularization

<http://neuralnetworksanddeeplearning.com/chap3.html#regularization>

减少神经网络的规模，但是更深层更大的网络潜在有更强的学习能力。即使对于固定的神经网络和固定的训练集，仍然能够减少 overfitting。

Regularization 的 cost 偏向于让神经网络学习比较小的权重 w, 除非第一项的 C0 明显减小。公式中的 $\lamda$ 调整两项的相对重要程度，较小的 $\lamda$ 倾向让第一项 C0 最小化，较大的倾向权重之和。

计算偏导数（对 w 和 b)。

加入 Regularization 不仅减少了 overfitting, 还避免陷入局部最小点 (local Mninimum), 更容易重现实验结果。具体原因见 PDF。

在神经网络中，Regularized 网络更鼓励小的权重，小权重的情况下，x 一些随机的变化不会对神经网络造成太大影响，所以更小可能受到数据局部噪音的影响。Un-Regularized 的神经网络，权重更大，容易通过神经网络模型比较大的改变来适应数据，更容易学习到局部数据的噪音。Regularized 更倾向于学到更简单一些的模型。


简单的模型并不一定总是最好，要从大量数据实验中获得，目前添加 Regularization 可以更好的泛化更多的实验中得来，理论的支持还在研究之中。


###L1 Regularization

公式和 L2 Regularization 相似，但是不一样。

然后求偏导等看 PDF 加深理解。

> sgn(w): sign of w


对比 L1 和 L2 的方法：

L1 减少一个常量，L2 减少权重的一个固定比例。
如果权重本身很大，L1 减少的比 L2 少很多；如果权重本身很小，L1 减少的更多。

> 一个是减去常量；一个是减去一个固定的比例。L1 倾向于集中在少部分重要的连接上。
> 注意 w=0 的情况，偏导数无意义，因为其形状是一个 V 字形拐点。这时使用 un-Regularized 表达式，sgn(w) = 0, 本来 Regularization 的目的就是减少权重，当权重等于 0 时，无需减少。



###Dropout

和 L1, L2 Regularization 非常不同，不是针对 cost 函数增加一项，而是对神经网络本身的结构做改变。

- 开始删除掉隐藏层随机选取的一半神经元（屏蔽掉）;
- 然后在这个更改过的神经网络上正向和反向更新，利用一个 mini-batch
- 然后恢复之前删除的神经元，重新随机选择一半神经元删除，正向和反向更新 w 和 b
- 重复此过程
- 最后学习出来的神经网络中的每个神经元都是只有一半神经元的基础上学习的（学到的 w 会偏大）, 当所有神经元恢复以后，为了补偿，我们把隐藏层的所有权重减半


为什么 dropout 能减少 overfitting

假设我们对于同一组训练数据，利用不同的神经网络来训练，训练完成之后，求输出的平均值，这样可以减少 overfitting。
Dropout 和这个是同一个道理，每次扔掉一般隐藏层的神经元，相当于我们在不同的神经网络上训练。减少了神经元的依赖性，也就是每个神经元不能依赖于某个或者其他神经元，迫使神经元联合起来更加健硕的特征。

> 对于 dropout, 对于以前 mnist 最高的 accuracy 是 98.4%, 利用 dropout 提高到了 98.7%


增大训练集，旋转拉伸等等。这可以增加很多训练数据，非常实用。

比如 MNIST, 一个隐藏层有 800 个神经元的网络准确率 98.4%, 人工增加数据后，达到 98.9%, 发明了一下人工会改变图像的模拟方法进一步增大训练集，准确率达到了 99.3%。增大时，要模拟现实世界中这种数据可能出现的变化，来概括更广。

比较 SVM 和 NN 算法的时候，是否使用同样的训练集很重要，因为训练集很大可以提高 accuracy。


###Weight Initialize

在 PDF 中的初始化实例中，500 个 1 和 500 个 0 导致 z 远远大于 1 或远远小于 -1。根据 sigmoid 函数的性质，输出的值都接近 0 和 1, 当权重变化时，更新量很小，对于更新后面的层，更新量很小，学习很慢。**这意味着隐藏层饱和，和之前说的输出层饱和问题相似，对于输出层，我们用改进的 cost 函数，** 比如 cross-entropy, 但是对于隐藏层，我们无法通过 cost 函数来改进。


这使得我们需要用更好的方法来初始化权重？从正态分布均值 =0, 标准差等于 1/sqrt(n_in)。重复 500 个 1 和 500 个 0 的时候，z 分布的标准差变成了 sqrt(3/2) = 1.22。
这样就可以保证大部分的 z 都在 -1 和 1 之间，不会导致隐藏层饱和。





















