##Softmax

<http://neuralnetworksanddeeplearning.com/chap3.html>

另一种类型的输出层的方程：

第一步和之前的 sigmoid 一样，还是加权求和再加上偏向。
第二步和之前的 sigmoid 不一样，使用了 softmax 函数。

在网站上有很有趣的 demo。可以发现增大其中的一个 z, 输出 a 的对应标号的增大，其余减小。

事实上，挪动 a4 的时候，其他 a 减小的值总是刚好等于 a4 增加的值，总和为 1 保持不变。（很简单的证明）


softmax 的输出值都是大于 0 的，且总和等于 1。所以可以认为是概率分布。**即可以认为输出的是分类等于每个可能分类标签的概率。**

如果输出层是 sigmoid 层，不能默认输出总和为 1, 所以不能轻易描述为概率分布。

定义 log-likelyhood 函数 (-ln)。查阅 PDF 可以看到概率小对应的对数 C 就比较大，所以适合做 Cost 函数。

> 是否存在学习慢的问题？
> 计算偏导可以发现和 cross-entropy 类似，错误大的时候偏导值大，这也意味着不会出现学习慢的问题。


<hr>

overfitting: 在训练集上表现很好，但是不能泛化到测试集上，测试集表现不好。

这在神经网络中是一个很重要的问题，神经网络有很多参数：隐藏层有 30 个神经元的神经网络有大约 24000 个参数。隐藏层有 100 个神经元的神经网络有约 80000 个参数，一些最新的神经网络甚至上亿个参数。

实例 demo 在 PDF 中查阅，解释的非常详尽

解决overfitting可以用 validation_data , 每一轮可以在validation_data上面进行测试(计算分类准确率), 收敛之后就停止训练。

当然增加训练集可以帮助减少overfitting。


